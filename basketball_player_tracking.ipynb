{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "- Import libraries\n",
        "- Download required files\n",
        "  - YOLO model (weights, classes, config)"
      ],
      "metadata": {
        "id": "BgeuPPLsxWZN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HG8auY0rBKCY"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/arunponnusamy/object-detection-opencv.git\n",
        "!wget https://pjreddie.com/media/files/yolov3.weights"
      ],
      "metadata": {
        "id": "5ly8rcDewIQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Model\n",
        "- Run YOLO pre-trained model to detect humans per frame of sample video\n",
        "   - WIP: Filtering out non-players (by location on court)\n",
        "- Write results to new video\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3ftP1hJXwoEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setupModel(class_file, weights_file, config_file):\n",
        "  # read class names from text file\n",
        "  classes = None\n",
        "  with open(class_file, 'r') as f:\n",
        "      classes = [line.strip() for line in f.readlines()]\n",
        "  # generate different colors for different classes \n",
        "  COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
        "\n",
        "  # read pre-trained model and config file\n",
        "  net = cv2.dnn.readNet(weights_file, config_file)\n",
        "\n",
        "  return (net, classes, COLORS)\n",
        "\n",
        "def processFrame(frame, net, classes, COLORS):\n",
        "  Width = frame.shape[1]\n",
        "  Height = frame.shape[0]\n",
        "  scale = 0.00392\n",
        "\n",
        "  # create input blob \n",
        "  blob = cv2.dnn.blobFromImage(frame, scale, (416,416), (0,0,0), True, crop=False)\n",
        "\n",
        "  # set input blob for the network\n",
        "  net.setInput(blob)\n",
        "\n",
        "  layer_names = net.getLayerNames()\n",
        "  output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
        "  outs = net.forward(output_layers)\n",
        "\n",
        "  class_ids = []\n",
        "  confidences = []\n",
        "  boxes = []\n",
        "  conf_threshold = 0.5\n",
        "  nms_threshold = 0.4\n",
        "\n",
        "  for out in outs:\n",
        "      for detection in out:\n",
        "          scores = detection[5:]\n",
        "          class_id = np.argmax(scores)\n",
        "          confidence = scores[class_id]\n",
        "          center_x = int(detection[0] * Width)\n",
        "          center_y = int(detection[1] * Height)\n",
        "          w = int(detection[2] * Width)\n",
        "          h = int(detection[3] * Height)\n",
        "          x = center_x - w / 2\n",
        "          y = center_y - h / 2\n",
        "          class_ids.append(class_id)\n",
        "          confidences.append(float(confidence))\n",
        "          boxes.append([x, y, w, h])\n",
        "\n",
        "\n",
        "  indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
        "\n",
        "  for i in indices:\n",
        "      box = boxes[i]\n",
        "      x = box[0]\n",
        "      y = box[1]\n",
        "      w = box[2]\n",
        "      h = box[3]\n",
        "      round(x), round(y), round(x+w), round(y+h)\n",
        "      label = str(classes[class_id])\n",
        "      color = COLORS[class_id]\n",
        "      cv2.rectangle(frame, (round(x),round(y)), (round(x+w),round(y+h)), color, 2)\n",
        "      cv2.putText(frame, label, (round(x)-10,round(y)-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "\n",
        "\n",
        "# Initialize video writer to write output video\n",
        "out = cv2.VideoWriter('output_video.avi',cv2.VideoWriter_fourcc(*'MJPG'), 30, (1280, 720))\n",
        "\n",
        "# Initialize video capture to process video\n",
        "cap = cv2.VideoCapture('/content/sample_video.mp4')\n",
        "\n",
        "# Setup net\n",
        "(net, classes, COLORS) = setupModel('/content/object-detection-opencv/yolov3.txt', '/content/yolov3.weights', '/content/object-detection-opencv/yolov3.cfg')\n",
        "\n",
        "# Iterate through frames of video\n",
        "n = 0\n",
        "while cap.isOpened():\n",
        "  # Get the current frame\n",
        "  ret, frame = cap.read()\n",
        "\n",
        "  # If the frame is None (video has ended), exit\n",
        "  if not ret or frame is None:\n",
        "    break\n",
        "  processFrame(frame, net, classes, COLORS)\n",
        "  # cv2_imshow(frame)\n",
        "  out.write(frame)  \n",
        "  n += 1\n",
        "  # Debug statement\n",
        "  # print(f\"Processed frame #{n}\")\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "print('done')"
      ],
      "metadata": {
        "id": "cwcJphPfheSj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}